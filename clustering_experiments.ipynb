{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "data_path = os.getcwd() + '/data'\n",
    "# demo bob\n",
    "mldata_demo_bob_path = data_path + '/mldata_demo_bob'\n",
    "# blazeface\n",
    "mldata_full_laurens_blazeface_path = data_path + '/ente_mldata_full_laurens_blazeface'\n",
    "# yolo \n",
    "mldata_full_laurens_yolo_path = data_path + '/ente_mldata_full_laurens_yolo_70'\n",
    "# mldata_full_laurens_yolo_path = data_path + '/ente-mldata-yolo-70-38000_FILES'\n",
    "lfw_demonstration_data_path = data_path + '/lfw_demonstration_data'\n",
    "\n",
    "# Define the data to use \n",
    "data_to_use = mldata_full_laurens_yolo_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT CHANGE ANYTHING HERE\n",
    "json_path = data_to_use + '/indexeddb/mldata.json'\n",
    "json_metadata_path = data_to_use + '/indexeddb/meta_data_10000025.json'\n",
    "\n",
    "with open(json_path, 'r') as file:\n",
    "    mldata_json = json.load(file)\n",
    "\n",
    "with open(json_metadata_path, 'r') as file:\n",
    "    mldata_metadata_json = json.load(file)\n",
    "\n",
    "indexed_files_json = mldata_json['files']\n",
    "clusters_web_json = mldata_json['library']\n",
    "people_web_json = mldata_json['people']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of files: 40577\n",
      "Amount of embeddings/faces: 42739\n",
      "Amount of low score faces: 11806\n"
     ]
    }
   ],
   "source": [
    "# get the embeddings from the indexed files\n",
    "low_score_faces = 0\n",
    "embeddings = []\n",
    "file_ids_of_embeddings = []\n",
    "face_ids_of_embeddings = []\n",
    "for file in indexed_files_json.values():\n",
    "    # skip files with errors\n",
    "    if 'faces' not in file:\n",
    "        continue\n",
    "    for face in file['faces']:\n",
    "        # skip faces with low detection confidence\n",
    "        if face['detection']['probability'] < face_detection_threshold:\n",
    "            low_score_faces += 1\n",
    "            continue\n",
    "        embeddings.append(face['embedding'])\n",
    "        file_ids_of_embeddings.append(str(file['fileId']))  # Convert file ID to string\n",
    "        face_ids_of_embeddings.append(str(face['id']))\n",
    "\n",
    "# Sort the embeddings, file_ids_of_embeddings, and face_ids_of_embeddings based on creationTime\n",
    "embeddings, file_ids_of_embeddings, face_ids_of_embeddings = zip(*sorted(zip(embeddings, file_ids_of_embeddings, face_ids_of_embeddings), key=lambda x: mldata_metadata_json[x[1]]['creationTime'], reverse=False))\n",
    "\n",
    "print(f\"Amount of files: {len(indexed_files_json.values())}\")\n",
    "print(f\"Amount of embeddings/faces: {len(embeddings)}\")\n",
    "print(f\"Amount of low score faces: {low_score_faces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if all embeddings are unique\n",
    "# for idx, embedding in enumerate(embeddings):\n",
    "#     if embeddings.count(embedding) > 1:\n",
    "#         print(f\"embedding of index {idx}: [{embedding[0]}, {embedding[1]}...{embedding[-1]}] is not unique\")\n",
    "\n",
    "# check if all entries in face_ids_of_embeddings are unique\n",
    "# for id in face_ids_of_embeddings:\n",
    "#     if face_ids_of_embeddings.count(id) > 1:\n",
    "#         print(f\"face_id {id} is not unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of cluster entries: 54545\n",
      "Amount of clusters: 520\n"
     ]
    }
   ],
   "source": [
    "# get the clusters from the clusters_web_json\n",
    "clusters_int_web = []\n",
    "clusters_fileids_web = []\n",
    "for cluster in clusters_web_json['data']['faceClusteringResults']['clusters']:\n",
    "    clusters_int_web.append(cluster)\n",
    "noise_web = clusters_web_json['data']['faceClusteringResults']['noise'] \n",
    "for cluster in people_web_json.values():\n",
    "    clusters_fileids_web.append(cluster['files'])\n",
    "\n",
    "cluster_entries_number = len(noise_web)\n",
    "for cluster in clusters_fileids_web:\n",
    "    cluster_entries_number += len(cluster)\n",
    "print(f\"Amount of cluster entries: {cluster_entries_number}\")\n",
    "print(f\"Amount of clusters: {len(clusters_int_web)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters_fileids_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42739"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_ids_of_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42739"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(face_ids_of_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear clustering\n",
    "\n",
    "This is based on simply comparing cosine distances, similar to Immich. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "# embeddings_cosine_distances = cosine_distances(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_cosine_distance = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_normalized_embeddings(embeddings_matrix):\n",
    "    # Normalize the embeddings_matrix\n",
    "    embeddings_matrix_norm = np.linalg.norm(embeddings_matrix, axis=1)\n",
    "    normalized_embeddings_matrix = embeddings_matrix / embeddings_matrix_norm[:, np.newaxis]\n",
    "    return normalized_embeddings_matrix\n",
    "\n",
    "def calculate_cosine_distance_using_normalized_embedding(normalized_embedding, normalized_embeddings_matrix):\n",
    "    # Calculate the dot product between the normalized embedding and normalized embeddings_matrix\n",
    "    cosine_similarity = np.dot(normalized_embeddings_matrix, normalized_embedding)\n",
    "    \n",
    "    # Calculate the cosine distance\n",
    "    cosine_distance = 1 - cosine_similarity\n",
    "    \n",
    "    return cosine_distance\n",
    "\n",
    "def calculate_cosine_distance_using_normalized_embedding_matrix(normalized_embeddings_matrix):\n",
    "    # Calculate the dot product between the normalized embedding and normalized embeddings_matrix\n",
    "    cosine_similarity = np.dot(normalized_embeddings_matrix, normalized_embeddings_matrix.T)\n",
    "    \n",
    "    # Calculate the cosine distance\n",
    "    cosine_distance = 1 - cosine_similarity\n",
    "    \n",
    "    return cosine_distance\n",
    "\n",
    "def calculate_cosine_distance_between_two_normalized_embedding_matrices(normalized_embeddings_matrix_1, normalized_embeddings_matrix_2):\n",
    "    # Calculate the dot product between the normalized embedding and normalized embeddings_matrix\n",
    "    cosine_similarity = np.dot(normalized_embeddings_matrix_1, normalized_embeddings_matrix_2.T)\n",
    "    \n",
    "    # Calculate the cosine distance\n",
    "    cosine_distance = 1 - cosine_similarity\n",
    "    \n",
    "    return cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "test = [x / 100.0 for x in range(10, 55, 5)]\n",
    "print(test)\n",
    "print(type(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42738it [00:59, 717.25it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the cluster assignments\n",
    "cluster_counter = 0\n",
    "cluster_results = np.zeros(len(embeddings), dtype=int)\n",
    "\n",
    "normalized_embeddings = calculate_normalized_embeddings(embeddings)\n",
    "\n",
    "# Iterate over each embedding, skipping the first one\n",
    "cluster_counter += 1\n",
    "cluster_results[0] = cluster_counter\n",
    "for idx, norm_embedding in tqdm(enumerate(normalized_embeddings[1:], start=1)):\n",
    "    # Calculate the cosine distances between the embedding and all other embeddings\n",
    "    # distances = cosine_distances([embedding], embeddings[:idx])[0]\n",
    "    distances = calculate_cosine_distance_using_normalized_embedding(norm_embedding, normalized_embeddings[:idx])\n",
    "    # distances = embeddings_cosine_distances[idx]\n",
    "    \n",
    "    # Find the index of the closest embedding\n",
    "    closest_idx = np.argmin(distances)\n",
    "    \n",
    "    # Get the cosine distance to the closest embedding\n",
    "    closest_distance = distances[closest_idx]\n",
    "    \n",
    "    # Check if the distance is below the threshold\n",
    "    if closest_distance < threshold_cosine_distance:\n",
    "        other_embedding_cluster = cluster_results[closest_idx]\n",
    "        # if other_embedding_cluster == 0:\n",
    "        #     cluster_counter += 1\n",
    "        #     cluster_results[idx] = cluster_counter\n",
    "        #     # cluster_results[closest_idx] = cluster_counter\n",
    "        #     print('TEST: do we ever get here?')\n",
    "        # else:\n",
    "        cluster_results[idx] = other_embedding_cluster\n",
    "    else:\n",
    "        cluster_counter += 1\n",
    "        cluster_results[idx] = cluster_counter\n",
    "\n",
    "# fill in the clusters list\n",
    "clusters = []\n",
    "for cluster_number in range(1, cluster_counter + 1):\n",
    "    cluster = []\n",
    "    for idx, cluster_result in enumerate(cluster_results):\n",
    "        if cluster_result == cluster_number:\n",
    "            cluster.append(idx)\n",
    "    clusters.append(cluster)\n",
    "\n",
    "clusters_larger_than_1 = [cluster for cluster in clusters if len(cluster) > 1]\n",
    "clusters_larger_than_2 = [cluster for cluster in clusters if len(cluster) > 2]\n",
    "# clusters_larger_than_1\n",
    "\n",
    "# get the file_ids of the clusters\n",
    "clusters_fileids = copy.deepcopy(clusters)\n",
    "clusters_faceids = copy.deepcopy(clusters)\n",
    "for cluster in clusters_fileids:\n",
    "    for idx, file_id in enumerate(cluster):\n",
    "        cluster[idx] = file_ids_of_embeddings[file_id]\n",
    "for cluster in clusters_faceids:\n",
    "    for idx, face_id in enumerate(cluster):\n",
    "        cluster[idx] = face_ids_of_embeddings[face_id]\n",
    "\n",
    "# get the clusters with more than 1 file_id\n",
    "clusters_fileids_larger_than_1 = [cluster for cluster in clusters_fileids if len(cluster) > 1]\n",
    "clusters_fileids_larger_than_2 = [cluster for cluster in clusters_fileids if len(cluster) > 2]\n",
    "\n",
    "# get the clusters with more than 1 face_id\n",
    "clusters_faceids_larger_than_1 = [cluster for cluster in clusters_faceids if len(cluster) > 1]\n",
    "clusters_faceids_larger_than_2 = [cluster for cluster in clusters_faceids if len(cluster) > 2]\n",
    "\n",
    "# clusters thumbnails by adding `.jpg` to every face_id\n",
    "clusters_thumbnails = copy.deepcopy(clusters_faceids)\n",
    "for cluster in clusters_thumbnails:\n",
    "    for idx, face_id in enumerate(cluster):\n",
    "        cluster[idx] = face_id + '.jpg'\n",
    "clusters_thumbnails_larger_than_1 = [cluster for cluster in clusters_thumbnails if len(cluster) > 1]\n",
    "clusters_thumbnails_larger_than_2 = [cluster for cluster in clusters_thumbnails if len(cluster) > 2]\n",
    "clusters_thumbnails_larger_than_10 = [cluster for cluster in clusters_thumbnails if len(cluster) > 10]\n",
    "clusters_thumbnails_larger_than_20 = [cluster for cluster in clusters_thumbnails if len(cluster) > 20]\n",
    "clusters_thumbnails_larger_than_30 = [cluster for cluster in clusters_thumbnails if len(cluster) > 30]\n",
    "clusters_thumbnails_larger_than_40 = [cluster for cluster in clusters_thumbnails if len(cluster) > 40]\n",
    "clusters_thumbnails_larger_than_50 = [cluster for cluster in clusters_thumbnails if len(cluster) > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking final clusters with more than 2 faces\n",
    "final_clusters = sorted(clusters_larger_than_2, key=len, reverse=True)\n",
    "final_clusters_fileids = sorted(clusters_fileids_larger_than_2, key=len, reverse=True)\n",
    "final_clusters_faceids = sorted(clusters_faceids_larger_than_2, key=len, reverse=True)\n",
    "final_clusters_thumbnails = sorted(clusters_thumbnails_larger_than_2, key=len, reverse=True)\n",
    "final_clusters_embeddings = sorted(clusters_larger_than_2, key=len, reverse=True)\n",
    "face_ids_to_cluster_map = {}\n",
    "for idx, cluster in enumerate(final_clusters_faceids):\n",
    "    for face_id in cluster:\n",
    "        face_ids_to_cluster_map[face_id] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clusters: 12899\n",
      "Number of clusters with more than 1 face_id: 3464\n",
      "Number of clusters with more than 2 face_id: 1405\n",
      "Number of clusters with more than 10 face_id: 175\n",
      "Number of clusters with more than 20 face_id: 91\n",
      "Number of clusters with more than 30 face_id: 67\n",
      "Number of clusters with more than 40 face_id: 62\n",
      "Number of clusters with more than 50 face_id: 53\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of clusters: {len(clusters_thumbnails)}\")\n",
    "print(f\"Number of clusters with more than 1 face_id: {len(clusters_thumbnails_larger_than_1)}\")\n",
    "print(f\"Number of clusters with more than 2 face_id: {len(clusters_thumbnails_larger_than_2)}\")\n",
    "print(f\"Number of clusters with more than 10 face_id: {len(clusters_thumbnails_larger_than_10)}\")\n",
    "print(f\"Number of clusters with more than 20 face_id: {len(clusters_thumbnails_larger_than_20)}\")\n",
    "print(f\"Number of clusters with more than 30 face_id: {len(clusters_thumbnails_larger_than_30)}\")\n",
    "print(f\"Number of clusters with more than 40 face_id: {len(clusters_thumbnails_larger_than_40)}\")\n",
    "print(f\"Number of clusters with more than 50 face_id: {len(clusters_thumbnails_larger_than_50)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster sizes of first 200 clusters: [6362 2167 1557 1311 1099  856  773  599  519  501  489  472  461  291\n",
      "  279  259  256  181  178  170  169  156  140  138  122  107  106  102\n",
      "  100   99   99   98   97   85   84   80   77   76   75   72   68   65\n",
      "   64   64   63   62   60   57   56   55   55   53   51   50   49   47\n",
      "   47   46   44   42   42   41   37   37   34   33   32   30   29   29\n",
      "   28   28   27   27   27   26   26   25   25   25   24   23   23   23\n",
      "   22   21   21   21   21   21   21   20   20   20   19   19   19   19\n",
      "   18   18   18   18   18   17   17   17   17   16   16   16   16   16\n",
      "   16   16   16   16   15   15   15   15   15   15   15   15   14   14\n",
      "   14   14   14   14   14   14   14   14   13   13   13   13   13   13\n",
      "   13   13   13   13   13   13   12   12   12   12   12   12   12   12\n",
      "   12   12   12   12   12   11   11   11   11   11   11   11   11   11\n",
      "   11   11   11   11   11   11   11   10   10   10   10   10   10   10\n",
      "   10   10   10   10   10   10   10   10   10   10   10   10   10   10\n",
      "   10   10   10    9]\n"
     ]
    }
   ],
   "source": [
    "cluster_size = []\n",
    "for cluster in final_clusters:\n",
    "    cluster_size.append(len(cluster))\n",
    "# cluster_size.sort(reverse=True)\n",
    "print(f\"Cluster sizes of first 200 clusters: {np.array(cluster_size[:200], dtype=int)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating folders with thumbnails of the new clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clusters_path = data_to_use + '/caches/new_clusters'\n",
    "new_clusters_folder = new_clusters_path + '/thumbnails_clusters'\n",
    "thumbnails_path = data_to_use + '/caches/face-crops'\n",
    "\n",
    "# Create the new_clusters_folder if it doesn't exist\n",
    "if not os.path.exists(new_clusters_folder):\n",
    "    os.makedirs(new_clusters_folder)\n",
    "else: # empty the directory if it already exists\n",
    "    shutil.rmtree(new_clusters_folder)\n",
    "    os.makedirs(new_clusters_folder)\n",
    "\n",
    "# Iterate over each cluster in final_clusters_thumbnails\n",
    "for cluster_idx, cluster in enumerate(final_clusters_thumbnails):\n",
    "    # Create a new folder for the cluster\n",
    "    cluster_folder = os.path.join(new_clusters_folder, f'cluster_{cluster_idx+1}_size_{len(cluster)}')\n",
    "    os.makedirs(cluster_folder)\n",
    "    \n",
    "    # Copy the thumbnails from the cluster to the cluster folder\n",
    "    for thumbnail_filename in cluster:\n",
    "        thumbnail_path = os.path.join(thumbnails_path, thumbnail_filename)\n",
    "        shutil.copy(thumbnail_path, cluster_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second stage improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding trash clusters\n",
    "\n",
    "Approaches to try:\n",
    "- [x] Computing the distances among the entries in a cluster, flag cluster if the average is high\n",
    "- [] Computing the 10 most extreme distances in a cluster, flag cluster if the sum is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_trash_clustes = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_ids_to_normalized_embeddings = {}\n",
    "for idx, face_id in enumerate(face_ids_of_embeddings):\n",
    "    face_ids_to_normalized_embeddings[face_id] = normalized_embeddings[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding trash clusters\n",
    "\n",
    "# first approach: computing the distances among the entries in a cluster, and flag if the average is above a threshold \n",
    "if find_trash_clustes:\n",
    "    average_intra_cluster_distances = []\n",
    "    for cluster in tqdm(clusters_faceids_larger_than_2):\n",
    "        cluster_normalized_embeddings = []\n",
    "        for face_id in cluster:\n",
    "            cluster_normalized_embeddings.append(face_ids_to_normalized_embeddings[face_id])\n",
    "        cluster_normalized_embeddings = np.array(cluster_normalized_embeddings)\n",
    "        cluster_embeddings_distances = calculate_cosine_distance_using_normalized_embedding_matrix(cluster_normalized_embeddings)\n",
    "        average_intra_cluster_distances.append(np.mean(cluster_embeddings_distances))\n",
    "    sorted_average_intra_cluster_distances_indices = np.argsort(average_intra_cluster_distances)[::-1]\n",
    "    print(f\"First approach: computing the distances among the entries in a cluster, and flag if the average is above a threshold:\")\n",
    "    print(f\"The 10 clusters with the highest average intra cluster distances are: {sorted_average_intra_cluster_distances_indices[:10]} \\n with average intra cluster distances: {np.array(average_intra_cluster_distances)[sorted_average_intra_cluster_distances_indices[:10]]} \\n \\n \\n\")\n",
    "\n",
    "    # second approach: computing the 10 most extreme distances in a cluster, and flag if the sum is above a threshold\n",
    "    sum_of_extreme_distances = []\n",
    "    for cluster in tqdm(clusters_faceids_larger_than_2):\n",
    "        cluster_normalized_embeddings = []\n",
    "        for face_id in cluster:\n",
    "            cluster_normalized_embeddings.append(face_ids_to_normalized_embeddings[face_id])\n",
    "        cluster_normalized_embeddings = np.array(cluster_normalized_embeddings)\n",
    "        cluster_embeddings_distances = calculate_cosine_distance_using_normalized_embedding_matrix(cluster_normalized_embeddings)\n",
    "        cluster_embeddings_distances = np.sort(cluster_embeddings_distances, axis=None)[::-1]\n",
    "        sum_of_extreme_distances.append(np.sum(cluster_embeddings_distances[:10]))\n",
    "    sorted_sum_of_extreme_distances_indices = np.argsort(sum_of_extreme_distances)[::-1]\n",
    "    print(f\"Second approach: computing the 10 most extreme distances in a cluster, and flag if the sum is above a threshold:\")\n",
    "    print(f\"The 10 clusters with the highest sum of extreme distances are: {sorted_sum_of_extreme_distances_indices[:10]} \\n with sum of extreme distances: {np.array(sum_of_extreme_distances)[sorted_sum_of_extreme_distances_indices[:10]]} \\n \\n \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting and filter low quality faces\n",
    "\n",
    "Filter rules for faces we should consider here:\n",
    "- [X] set threshold for yolo confidence to `0.8`\n",
    "- [X] sideways faces using landmarks\n",
    "- [X] rotated faces\n",
    "- [X] low resolution faces using bounding box plus image size\n",
    "- [X] blur using FFT on aligned face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_faces = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_filters_path = data_to_use + '/caches/face_filters'\n",
    "\n",
    "# Create the face_filters_path if it doesn't exist\n",
    "if not os.path.exists(face_filters_path):\n",
    "    os.makedirs(face_filters_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of faces with detection score between 70 and 75: 5023\n",
      "Amount of faces with detection score between 75 and 80: 6783\n",
      "Amount of faces with detection score between 70 and 80: 11806\n"
     ]
    }
   ],
   "source": [
    "if filter_faces:\n",
    "    # Filter faces with low detection confidence\n",
    "    score_70_75_path = face_filters_path + '/score_70_75'\n",
    "    score_75_80_path = face_filters_path + '/score_75_80'\n",
    "\n",
    "    # Create the score_70_75_path and score_75_80_path if they don't exist\n",
    "    if not os.path.exists(score_70_75_path):\n",
    "        os.makedirs(score_70_75_path)\n",
    "    else:\n",
    "        shutil.rmtree(score_70_75_path)\n",
    "        os.makedirs(score_70_75_path)\n",
    "    if not os.path.exists(score_75_80_path):\n",
    "        os.makedirs(score_75_80_path)\n",
    "    else:\n",
    "        shutil.rmtree(score_75_80_path)\n",
    "        os.makedirs(score_75_80_path)\n",
    "\n",
    "    score_70_75_counter = 0\n",
    "    score_75_80_counter = 0\n",
    "    for file in indexed_files_json.values():\n",
    "        # skip files with errors\n",
    "        if 'faces' not in file:\n",
    "            continue\n",
    "        for face in file['faces']:\n",
    "            # skip faces with low detection confidence\n",
    "            if face['detection']['probability'] >= 0.7 and face['detection']['probability'] < 0.75:\n",
    "                face_path = os.path.join(thumbnails_path, face['id'] + '.jpg') \n",
    "                shutil.copy(face_path, score_70_75_path)\n",
    "                score_70_75_counter += 1\n",
    "            elif face['detection']['probability'] >= 0.75 and face['detection']['probability'] < 0.8:\n",
    "                face_path = os.path.join(thumbnails_path, face['id'] + '.jpg') \n",
    "                shutil.copy(face_path, score_75_80_path)\n",
    "                score_75_80_counter += 1\n",
    "    score_70_80_counter = score_70_75_counter + score_75_80_counter\n",
    "    print(f\"Amount of faces with detection score between 70 and 75: {score_70_75_counter}\")\n",
    "    print(f\"Amount of faces with detection score between 75 and 80: {score_75_80_counter}\")\n",
    "    print(f\"Amount of faces with detection score between 70 and 80: {score_70_80_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sideways_naive(facial_landmarks):\n",
    "    left_eye = facial_landmarks[0]\n",
    "    right_eye = facial_landmarks[1]\n",
    "    nose_tip = facial_landmarks[2]\n",
    "    # left_mouth_corner = facial_landmarks[3]\n",
    "    # right_mouth_corner = facial_landmarks[4]\n",
    "\n",
    "    eyes_height = (left_eye['y'] + right_eye['y']) / 2\n",
    "    nose_height = nose_tip['y']\n",
    "    eyes_to_nose_height = abs(eyes_height - nose_height)\n",
    "    eyes_width = abs(left_eye['x'] - right_eye['x'])\n",
    "\n",
    "    eyes_widht_to_nose_height_ratio = eyes_width / eyes_to_nose_height\n",
    "\n",
    "    if eyes_widht_to_nose_height_ratio < 0.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of sideways faces: 1804\n"
     ]
    }
   ],
   "source": [
    "if filter_faces:\n",
    "    # Filter sideways faces\n",
    "    sideways = face_filters_path + '/sideways'\n",
    "\n",
    "    # Create the sideways path if it doesn't exist\n",
    "    if not os.path.exists(sideways):\n",
    "        os.makedirs(sideways)\n",
    "    else:\n",
    "        shutil.rmtree(sideways)\n",
    "        os.makedirs(sideways)\n",
    "\n",
    "    sidways_counter = 0\n",
    "    sideways_face_ids = []\n",
    "    sideways_file_ids = []\n",
    "    for file in indexed_files_json.values():\n",
    "        # skip files with errors\n",
    "        if 'faces' not in file:\n",
    "            continue\n",
    "        for face in file['faces']:\n",
    "            # detect if face is sideways and copy it to the sideways folder if it is\n",
    "            if detect_sideways_naive(face['detection']['landmarks']) and face['detection']['probability'] >= 0.8:\n",
    "                face_path = os.path.join(thumbnails_path, face['id'] + '.jpg') \n",
    "                sideways_face_ids.append(face['id'])\n",
    "                sideways_file_ids.append(file['fileId'])\n",
    "                shutil.copy(face_path, sideways)\n",
    "                sidways_counter += 1\n",
    "    print(f\"Amount of sideways faces: {sidways_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of useful sideways faces: 496 out of 1804, which is 27.494456762749447%\n",
      "Amount of useful sideways faces in the checked clusters: [34 16 22  1  2  3  0  6  1  1  0  6  3 51  0  0  0 13  1  0 59  2 43  0\n",
      "  0  2  0  0  0  1  0  0  0  0  0  0  0 57  0  0  0  0  8  2  0  0  0 46\n",
      "  0  0  3  0  0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
      "  0 18  3  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  3  0\n",
      "  0  0 16  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  7  3  0  0  9  9  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "if filter_faces:\n",
    "    sideways_faces_in_clusters = []\n",
    "    useful_sideways_faces_counter = 0\n",
    "    check_clusters = final_clusters_faceids[:200]\n",
    "    useful_sideways_faces_in_clusters = np.zeros(len(check_clusters), dtype=int)\n",
    "    for idx, cluster in enumerate(check_clusters):\n",
    "        sideways_faces = []\n",
    "        for face_id in cluster:\n",
    "            if face_id in sideways_face_ids:\n",
    "                sideways_faces.append(face_id)\n",
    "                useful_sideways_faces_counter += 1\n",
    "                useful_sideways_faces_in_clusters[idx] += 1\n",
    "        sideways_faces_in_clusters.append(sideways_faces)\n",
    "    print(f\"Amount of useful sideways faces: {useful_sideways_faces_counter} out of {sidways_counter}, which is {(useful_sideways_faces_counter / sidways_counter * 100)}%\")\n",
    "    print(f\"Amount of useful sideways faces in the checked clusters: {useful_sideways_faces_in_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_side = 40\n",
    "threshold_surface = threshold_side * threshold_side\n",
    "\n",
    "def detect_small_faces(box, threshold_surface = threshold_surface):\n",
    "    box_width = box['width']\n",
    "    box_height = box['height']\n",
    "\n",
    "    surface = box_width * box_height\n",
    "\n",
    "    if surface < threshold_surface:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of small faces (bounding box): 1827\n"
     ]
    }
   ],
   "source": [
    "# Filter faces based on small bounding box size\n",
    "if filter_faces:\n",
    "    # Filter sideways faces\n",
    "    small_bounding_box = face_filters_path + '/small_bounding_box'\n",
    "\n",
    "    # Create the sideways path if it doesn't exist\n",
    "    if not os.path.exists(small_bounding_box):\n",
    "        os.makedirs(small_bounding_box)\n",
    "    else:\n",
    "        shutil.rmtree(small_bounding_box)\n",
    "        os.makedirs(small_bounding_box)\n",
    "\n",
    "    small_bounding_box_counter = 0\n",
    "    small_bounding_box_face_ids = []\n",
    "    small_bounding_box_file_ids = []\n",
    "    for file in indexed_files_json.values():\n",
    "        # skip files with errors\n",
    "        if 'faces' not in file:\n",
    "            continue\n",
    "        for face in file['faces']:\n",
    "            # detect if face is sideways and copy it to the sideways folder if it is\n",
    "            if detect_small_faces(face['detection']['box']) and face['detection']['probability'] >= 0.8:\n",
    "                face_path = os.path.join(thumbnails_path, face['id'] + '.jpg') \n",
    "                small_bounding_box_face_ids.append(face['id'])\n",
    "                small_bounding_box_file_ids.append(file['fileId'])\n",
    "                shutil.copy(face_path, small_bounding_box)\n",
    "                small_bounding_box_counter += 1\n",
    "    print(f\"Amount of small faces (bounding box): {small_bounding_box_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of useful small_bounding_box faces: 263 out of 1827, which is 14.395183360700603%\n",
      "Amount of useful small_bounding_box faces in the checked clusters: [33  1  9 10  1  1  3 10  1  0  0  2  1 45  0  0  0 17  0  2  6  1 14  1\n",
      "  0  1  1  2  8  1  1  0  0  0  0  0  0  4  0  2  1  1  0 17  1  1  0  3\n",
      "  0  0 10  0  1  0  4  0  0  0  0  0  6  0  0  0  0  0  1  0  0  0  3  1\n",
      "  0  2  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0\n",
      "  0  1  0  0  0  0  1  0  1  2  0  0  0  0  0  0  0  0  0  0  0  0  3  0\n",
      "  0  0  1  0  0  0  0  0 10  0  0  0  0  0  4  0  0  0  0  0  0  0  0  1\n",
      "  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "if filter_faces:\n",
    "    small_bounding_box_faces_in_clusters = []\n",
    "    useful_small_bounding_box_faces_counter = 0\n",
    "    check_clusters = final_clusters_faceids[:200]\n",
    "    useful_small_bounding_box_faces_in_clusters = np.zeros(len(check_clusters), dtype=int)\n",
    "    for idx, cluster in enumerate(check_clusters):\n",
    "        small_bounding_box_faces = []\n",
    "        for face_id in cluster:\n",
    "            if face_id in small_bounding_box_face_ids:\n",
    "                small_bounding_box_faces.append(face_id)\n",
    "                useful_small_bounding_box_faces_counter += 1\n",
    "                useful_small_bounding_box_faces_in_clusters[idx] += 1\n",
    "        small_bounding_box_faces_in_clusters.append(small_bounding_box_faces)\n",
    "    print(f\"Amount of useful small_bounding_box faces: {useful_small_bounding_box_faces_counter} out of {small_bounding_box_counter}, which is {(useful_small_bounding_box_faces_counter / small_bounding_box_counter * 100)}%\")\n",
    "    print(f\"Amount of useful small_bounding_box faces in the checked clusters: {useful_small_bounding_box_faces_in_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cluster_average_image_size(clusters_fileids, metadata_fileids):\n",
    "    cluster_average_image_size = np.zeros(len(clusters_fileids), dtype=int)\n",
    "    for idx, cluster in enumerate(clusters_fileids):\n",
    "        sum_size = 0\n",
    "        size_counter = 0\n",
    "        for file_id in cluster:\n",
    "            if metadata_fileids[file_id]['size'] != 0:\n",
    "                sum_size += metadata_fileids[file_id]['size']\n",
    "                size_counter += 1\n",
    "        if size_counter != 0:\n",
    "            cluster_average_image_size[idx] = sum_size / size_counter\n",
    "\n",
    "    return cluster_average_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_average_image_size = calculate_cluster_average_image_size(final_clusters_fileids, mldata_metadata_json)\n",
    "clusters_average_image_size[:200]/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of faces from small images: 8162\n"
     ]
    }
   ],
   "source": [
    "# Filter faces based on low image size\n",
    "if filter_faces:\n",
    "    # Filter sideways faces\n",
    "    small_image = face_filters_path + '/small_image'\n",
    "\n",
    "    # Create the sideways path if it doesn't exist\n",
    "    if not os.path.exists(small_image):\n",
    "        os.makedirs(small_image)\n",
    "    else:\n",
    "        shutil.rmtree(small_image)\n",
    "        os.makedirs(small_image)\n",
    "\n",
    "    small_image_counter = 0\n",
    "    small_image_face_ids = []\n",
    "    small_image_file_ids = []\n",
    "    for file in indexed_files_json.values():\n",
    "        # skip files with errors\n",
    "        if 'faces' not in file:\n",
    "            continue\n",
    "        if mldata_metadata_json[str(file['fileId'])]['size'] < 100000:\n",
    "            for face in file['faces']:\n",
    "                # detect if face is sideways and copy it to the sideways folder if it is\n",
    "                if face['detection']['probability'] >= 0.8:\n",
    "                    face_path = os.path.join(thumbnails_path, face['id'] + '.jpg') \n",
    "                    small_image_face_ids.append(face['id'])\n",
    "                    small_image_file_ids.append(file['fileId'])\n",
    "                    shutil.copy(face_path, small_image)\n",
    "                    small_image_counter += 1\n",
    "    print(f\"Amount of faces from small images: {small_image_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter faces based on low image size and bounding box size ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second stage merging of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6362"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_clusters_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2167"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_clusters_embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42739, 192)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [00:02, 90.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 clusters with the lowest median distances to the first cluster are: [ 86  10   8 131 108 184 137 102  96 141] \n",
      " with median distances: [0.44778493 0.45415174 0.47975667 0.50488278 0.54223017 0.58516732\n",
      " 0.59617362 0.62022092 0.62487244 0.62797093] \n",
      " \n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's investigate which clusters can be merged with the first cluster\n",
    "embeddings_first_cluster = normalized_embeddings[final_clusters_embeddings[0]]\n",
    "embeddings_second_cluster = normalized_embeddings[final_clusters_embeddings[1]]\n",
    "inner_cluster_distances_first_cluster = calculate_cosine_distance_using_normalized_embedding_matrix(embeddings_first_cluster)\n",
    "\n",
    "# Let's investigate which clusters can be merged with the first cluster\n",
    "embeddings_first_cluster = normalized_embeddings[final_clusters_embeddings[0]]\n",
    "median_distances = []\n",
    "for idx, cluster in tqdm(enumerate(final_clusters_embeddings[1:200])):\n",
    "    embeddings_second_cluster = normalized_embeddings[cluster]\n",
    "    all_distances_between_two_clusters = calculate_cosine_distance_between_two_normalized_embedding_matrices(embeddings_first_cluster, embeddings_second_cluster)\n",
    "    median_distance = np.median(all_distances_between_two_clusters.flatten())\n",
    "    median_distances.append(median_distance)\n",
    "\n",
    "# Now let's sort the median_distances and get the indices of the sorted median_distances\n",
    "sorted_median_distances_indices = np.argsort(median_distances)\n",
    "sorted_median_distances = np.array(median_distances)[sorted_median_distances_indices]\n",
    "print(f\"The 10 clusters with the lowest median distances to the first cluster are: {sorted_median_distances_indices[:10] +2} \\n with median distances: {sorted_median_distances[:10]} \\n \\n \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_recognition_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
