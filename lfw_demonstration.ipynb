{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "data_path = os.getcwd() + '/data'\n",
    "# demo bob\n",
    "mldata_demo_bob_path = data_path + '/mldata_demo_bob'\n",
    "# blazeface\n",
    "mldata_full_laurens_blazeface_path = data_path + '/ente_mldata_full_laurens_blazeface'\n",
    "# yolo \n",
    "mldata_full_laurens_yolo_path = data_path + '/ente_mldata_full_laurens_yolo_70'\n",
    "# mldata_full_laurens_yolo_path = data_path + '/ente-mldata-yolo-70-38000_FILES'\n",
    "lfw_demonstration_data_path = data_path + '/lfw_demonstration_data'\n",
    "\n",
    "# Define the data to use \n",
    "data_to_use = lfw_demonstration_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT CHANGE ANYTHING HERE\n",
    "json_path = data_to_use + '/indexeddb/mldata.json'\n",
    "json_metadata_path = data_to_use + '/indexeddb/meta_data_10000025.json'\n",
    "\n",
    "with open(json_path, 'r') as file:\n",
    "    mldata_json = json.load(file)\n",
    "\n",
    "indexed_files_json = mldata_json['files']\n",
    "clusters_web_json = mldata_json['library']\n",
    "people_web_json = mldata_json['people']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of files: 4350\n",
      "Amount of embeddings/faces: 4835\n",
      "Amount of low score faces: 603\n"
     ]
    }
   ],
   "source": [
    "# get the embeddings from the indexed files\n",
    "low_score_faces = 0\n",
    "embeddings = []\n",
    "file_ids_of_embeddings = []\n",
    "face_ids_of_embeddings = []\n",
    "for file in indexed_files_json.values():\n",
    "    # skip files with errors\n",
    "    if 'faces' not in file:\n",
    "        continue\n",
    "    for face in file['faces']:\n",
    "        # skip faces with low detection confidence\n",
    "        if face['detection']['probability'] < face_detection_threshold:\n",
    "            low_score_faces += 1\n",
    "            continue\n",
    "        embeddings.append(face['embedding'])\n",
    "        file_ids_of_embeddings.append(str(file['fileId']))  # Convert file ID to string\n",
    "        face_ids_of_embeddings.append(str(face['id']))\n",
    "\n",
    "# # Sort the embeddings, file_ids_of_embeddings, and face_ids_of_embeddings based on creationTime\n",
    "# if data_to_use != lfw_demonstration_data_path:\n",
    "#     embeddings, file_ids_of_embeddings, face_ids_of_embeddings = zip(*sorted(zip(embeddings, file_ids_of_embeddings, face_ids_of_embeddings), key=lambda x: mldata_metadata_json[x[1]]['creationTime'], reverse=False))\n",
    "\n",
    "print(f\"Amount of files: {len(indexed_files_json.values())}\")\n",
    "print(f\"Amount of embeddings/faces: {len(embeddings)}\")\n",
    "print(f\"Amount of low score faces: {low_score_faces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if all embeddings are unique\n",
    "# for idx, embedding in enumerate(embeddings):\n",
    "#     if embeddings.count(embedding) > 1:\n",
    "#         print(f\"embedding of index {idx}: [{embedding[0]}, {embedding[1]}...{embedding[-1]}] is not unique\")\n",
    "\n",
    "# check if all entries in face_ids_of_embeddings are unique\n",
    "# for id in face_ids_of_embeddings:\n",
    "#     if face_ids_of_embeddings.count(id) > 1:\n",
    "#         print(f\"face_id {id} is not unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of cluster entries: 5438\n",
      "Amount of clusters: 168\n"
     ]
    }
   ],
   "source": [
    "# get the clusters from the clusters_web_json\n",
    "clusters_int_web = []\n",
    "clusters_fileids_web = []\n",
    "for cluster in clusters_web_json['data']['faceClusteringResults']['clusters']:\n",
    "    clusters_int_web.append(cluster)\n",
    "noise_web = clusters_web_json['data']['faceClusteringResults']['noise'] \n",
    "for cluster in people_web_json.values():\n",
    "    clusters_fileids_web.append(cluster['files'])\n",
    "\n",
    "cluster_entries_number = len(noise_web)\n",
    "for cluster in clusters_fileids_web:\n",
    "    cluster_entries_number += len(cluster)\n",
    "print(f\"Amount of cluster entries: {cluster_entries_number}\")\n",
    "print(f\"Amount of clusters: {len(clusters_int_web)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters_fileids_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4835"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_ids_of_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4835"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(face_ids_of_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear clustering\n",
    "\n",
    "This is based on simply comparing cosine distances, similar to Immich. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "# embeddings_cosine_distances = cosine_distances(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_cosine_distance = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_normalized_embeddings(embeddings_matrix):\n",
    "    # Normalize the embeddings_matrix\n",
    "    embeddings_matrix_norm = np.linalg.norm(embeddings_matrix, axis=1)\n",
    "    normalized_embeddings_matrix = embeddings_matrix / embeddings_matrix_norm[:, np.newaxis]\n",
    "    return normalized_embeddings_matrix\n",
    "\n",
    "def calculate_cosine_distance_using_normalized_embedding(normalized_embedding, normalized_embeddings_matrix):\n",
    "    # Calculate the dot product between the normalized embedding and normalized embeddings_matrix\n",
    "    cosine_similarity = np.dot(normalized_embeddings_matrix, normalized_embedding)\n",
    "    \n",
    "    # Calculate the cosine distance\n",
    "    cosine_distance = 1 - cosine_similarity\n",
    "    \n",
    "    return cosine_distance\n",
    "\n",
    "def calculate_cosine_distance_using_normalized_embedding_matrix(normalized_embeddings_matrix):\n",
    "    # Calculate the dot product between the normalized embedding and normalized embeddings_matrix\n",
    "    cosine_similarity = np.dot(normalized_embeddings_matrix, normalized_embeddings_matrix.T)\n",
    "    \n",
    "    # Calculate the cosine distance\n",
    "    cosine_distance = 1 - cosine_similarity\n",
    "    \n",
    "    return cosine_distance\n",
    "\n",
    "def calculate_cosine_distance_between_two_normalized_embedding_matrices(normalized_embeddings_matrix_1, normalized_embeddings_matrix_2):\n",
    "    # Calculate the dot product between the normalized embedding and normalized embeddings_matrix\n",
    "    cosine_similarity = np.dot(normalized_embeddings_matrix_1, normalized_embeddings_matrix_2.T)\n",
    "    \n",
    "    # Calculate the cosine distance\n",
    "    cosine_distance = 1 - cosine_similarity\n",
    "    \n",
    "    return cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4834it [00:00, 34123.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the cluster assignments\n",
    "cluster_counter = 0\n",
    "cluster_results = np.zeros(len(embeddings), dtype=int)\n",
    "\n",
    "normalized_embeddings = calculate_normalized_embeddings(embeddings)\n",
    "\n",
    "# Iterate over each embedding, skipping the first one\n",
    "cluster_counter += 1\n",
    "cluster_results[0] = cluster_counter\n",
    "for idx, norm_embedding in tqdm(enumerate(normalized_embeddings[1:], start=1)):\n",
    "    # Calculate the cosine distances between the embedding and all other embeddings\n",
    "    # distances = cosine_distances([embedding], embeddings[:idx])[0]\n",
    "    distances = calculate_cosine_distance_using_normalized_embedding(norm_embedding, normalized_embeddings[:idx])\n",
    "    # distances = embeddings_cosine_distances[idx]\n",
    "    \n",
    "    # Find the index of the closest embedding\n",
    "    closest_idx = np.argmin(distances)\n",
    "    \n",
    "    # Get the cosine distance to the closest embedding\n",
    "    closest_distance = distances[closest_idx]\n",
    "    \n",
    "    # Check if the distance is below the threshold\n",
    "    if closest_distance < threshold_cosine_distance:\n",
    "        other_embedding_cluster = cluster_results[closest_idx]\n",
    "        # if other_embedding_cluster == 0:\n",
    "        #     cluster_counter += 1\n",
    "        #     cluster_results[idx] = cluster_counter\n",
    "        #     # cluster_results[closest_idx] = cluster_counter\n",
    "        #     print('TEST: do we ever get here?')\n",
    "        # else:\n",
    "        cluster_results[idx] = other_embedding_cluster\n",
    "    else:\n",
    "        cluster_counter += 1\n",
    "        cluster_results[idx] = cluster_counter\n",
    "\n",
    "# fill in the clusters list\n",
    "clusters = []\n",
    "for cluster_number in range(1, cluster_counter + 1):\n",
    "    cluster = []\n",
    "    for idx, cluster_result in enumerate(cluster_results):\n",
    "        if cluster_result == cluster_number:\n",
    "            cluster.append(idx)\n",
    "    clusters.append(cluster)\n",
    "\n",
    "clusters_larger_than_1 = [cluster for cluster in clusters if len(cluster) > 1]\n",
    "clusters_larger_than_2 = [cluster for cluster in clusters if len(cluster) > 2]\n",
    "# clusters_larger_than_1\n",
    "\n",
    "# get the file_ids of the clusters\n",
    "clusters_fileids = copy.deepcopy(clusters)\n",
    "clusters_faceids = copy.deepcopy(clusters)\n",
    "for cluster in clusters_fileids:\n",
    "    for idx, file_id in enumerate(cluster):\n",
    "        cluster[idx] = file_ids_of_embeddings[file_id]\n",
    "for cluster in clusters_faceids:\n",
    "    for idx, face_id in enumerate(cluster):\n",
    "        cluster[idx] = face_ids_of_embeddings[face_id]\n",
    "\n",
    "# get the clusters with more than 1 file_id\n",
    "clusters_fileids_larger_than_1 = [cluster for cluster in clusters_fileids if len(cluster) > 1]\n",
    "clusters_fileids_larger_than_2 = [cluster for cluster in clusters_fileids if len(cluster) > 2]\n",
    "\n",
    "# get the clusters with more than 1 face_id\n",
    "clusters_faceids_larger_than_1 = [cluster for cluster in clusters_faceids if len(cluster) > 1]\n",
    "clusters_faceids_larger_than_2 = [cluster for cluster in clusters_faceids if len(cluster) > 2]\n",
    "\n",
    "# clusters thumbnails by adding `.jpg` to every face_id\n",
    "clusters_thumbnails = copy.deepcopy(clusters_faceids)\n",
    "for cluster in clusters_thumbnails:\n",
    "    for idx, face_id in enumerate(cluster):\n",
    "        cluster[idx] = face_id + '.jpg'\n",
    "clusters_thumbnails_larger_than_1 = [cluster for cluster in clusters_thumbnails if len(cluster) > 1]\n",
    "clusters_thumbnails_larger_than_2 = [cluster for cluster in clusters_thumbnails if len(cluster) > 2]\n",
    "clusters_thumbnails_larger_than_10 = [cluster for cluster in clusters_thumbnails if len(cluster) > 10]\n",
    "clusters_thumbnails_larger_than_20 = [cluster for cluster in clusters_thumbnails if len(cluster) > 20]\n",
    "clusters_thumbnails_larger_than_30 = [cluster for cluster in clusters_thumbnails if len(cluster) > 30]\n",
    "clusters_thumbnails_larger_than_40 = [cluster for cluster in clusters_thumbnails if len(cluster) > 40]\n",
    "clusters_thumbnails_larger_than_50 = [cluster for cluster in clusters_thumbnails if len(cluster) > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking final clusters with more than 2 faces\n",
    "final_clusters = sorted(clusters_larger_than_2, key=len, reverse=True)\n",
    "final_clusters_fileids = sorted(clusters_fileids_larger_than_2, key=len, reverse=True)\n",
    "final_clusters_faceids = sorted(clusters_faceids_larger_than_2, key=len, reverse=True)\n",
    "final_clusters_thumbnails = sorted(clusters_thumbnails_larger_than_2, key=len, reverse=True)\n",
    "final_clusters_embeddings = sorted(clusters_larger_than_2, key=len, reverse=True)\n",
    "face_ids_to_cluster_map = {}\n",
    "for idx, cluster in enumerate(final_clusters_faceids):\n",
    "    for face_id in cluster:\n",
    "        face_ids_to_cluster_map[face_id] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clusters: 1000\n",
      "Number of clusters with more than 1 face_id: 237\n",
      "Number of clusters with more than 2 face_id: 188\n",
      "Number of clusters with more than 10 face_id: 115\n",
      "Number of clusters with more than 20 face_id: 49\n",
      "Number of clusters with more than 30 face_id: 28\n",
      "Number of clusters with more than 40 face_id: 15\n",
      "Number of clusters with more than 50 face_id: 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of clusters: {len(clusters_thumbnails)}\")\n",
    "print(f\"Number of clusters with more than 1 face_id: {len(clusters_thumbnails_larger_than_1)}\")\n",
    "print(f\"Number of clusters with more than 2 face_id: {len(clusters_thumbnails_larger_than_2)}\")\n",
    "print(f\"Number of clusters with more than 10 face_id: {len(clusters_thumbnails_larger_than_10)}\")\n",
    "print(f\"Number of clusters with more than 20 face_id: {len(clusters_thumbnails_larger_than_20)}\")\n",
    "print(f\"Number of clusters with more than 30 face_id: {len(clusters_thumbnails_larger_than_30)}\")\n",
    "print(f\"Number of clusters with more than 40 face_id: {len(clusters_thumbnails_larger_than_40)}\")\n",
    "print(f\"Number of clusters with more than 50 face_id: {len(clusters_thumbnails_larger_than_50)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster sizes of first 200 clusters: [521 219 127 104  96  70  69  59  51  50  48  47  47  45  44  39  39  39\n",
      "  37  35  34  34  34  33  33  32  31  31  30  29  28  27  26  26  26  26\n",
      "  25  25  23  23  23  22  22  22  21  21  21  21  21  20  20  20  19  19\n",
      "  18  17  17  17  17  16  16  16  16  16  16  16  16  15  15  15  15  15\n",
      "  15  14  14  14  14  14  14  14  14  14  14  14  13  13  13  13  13  13\n",
      "  13  13  12  12  12  12  12  12  12  12  12  12  12  12  11  11  11  11\n",
      "  11  11  11  11  11  11  11  10  10  10  10  10  10  10  10  10   9   9\n",
      "   9   9   9   9   9   9   9   9   9   9   9   9   8   8   8   8   8   8\n",
      "   8   8   8   8   7   7   7   7   7   7   7   7   7   7   6   6   6   6\n",
      "   6   6   5   5   5   5   4   4   4   4   4   4   4   4   4   4   4   3\n",
      "   3   3   3   3   3   3   3   3]\n"
     ]
    }
   ],
   "source": [
    "cluster_size = []\n",
    "for cluster in final_clusters:\n",
    "    cluster_size.append(len(cluster))\n",
    "# cluster_size.sort(reverse=True)\n",
    "print(f\"Cluster sizes of first 200 clusters: {np.array(cluster_size[:200], dtype=int)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating folders with thumbnails of the new clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clusters_path = data_to_use + '/caches/new_clusters'\n",
    "new_clusters_folder = new_clusters_path + '/thumbnails_clusters'\n",
    "thumbnails_path = data_to_use + '/caches/face-crops'\n",
    "\n",
    "# Create the new_clusters_folder if it doesn't exist\n",
    "if not os.path.exists(new_clusters_folder):\n",
    "    os.makedirs(new_clusters_folder)\n",
    "else: # empty the directory if it already exists\n",
    "    shutil.rmtree(new_clusters_folder)\n",
    "    os.makedirs(new_clusters_folder)\n",
    "\n",
    "# Iterate over each cluster in final_clusters_thumbnails\n",
    "for cluster_idx, cluster in enumerate(final_clusters_thumbnails):\n",
    "    # Create a new folder for the cluster\n",
    "    cluster_folder = os.path.join(new_clusters_folder, f'cluster_{cluster_idx+1}_size_{len(cluster)}')\n",
    "    os.makedirs(cluster_folder)\n",
    "    \n",
    "    # Copy the thumbnails from the cluster to the cluster folder\n",
    "    for thumbnail_filename in cluster:\n",
    "        thumbnail_path = os.path.join(thumbnails_path, thumbnail_filename)\n",
    "        shutil.copy(thumbnail_path, cluster_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_recognition_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
